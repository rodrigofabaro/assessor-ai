# Assessorâ€‘AI â€” Milestones

This is the â€œboring but reliableâ€ build tracker.

**Rule:** each milestone ends with a working UI path + database truth + audit trail.

Status labels:
- âœ… DONE
- ğŸŸ¨ IN PROGRESS
- ğŸ”œ NEXT
- ğŸ§Š PARKED

---

## âœ… M1 â€” Upload & Tracking Engine
**Outcome**
- Upload single + batch submissions (PDF/DOCX)
- Store files on disk + DB record per submission
- Submission list UI + status tracking

**Acceptance**
- Uploads create DB rows
- Files persist and can be re-opened
- Status transitions are consistent and visible

---

## ğŸŸ¨ M2 â€” Reference Library (Specs/Briefs)
**Outcome**
- Admin upload reference docs (unit specs, briefs)
- Parse + store structured reference data
- Bind assignments â†’ criteria universe

**Acceptance**
- Reference docs stored and re-usable
- Assignment binding exists and is queryable
- Locked reference versions are immutable

---

## ğŸŸ¨ M3 â€” Extraction Engine
**Outcome**
- Extract text from PDF/DOCX
- Perâ€‘page extraction stored separately from raw file
- Preview UI shows pages + extracted text
- Confidence/meaningfulâ€‘text guards prevent nonsense

**Acceptance**
- Submission detail page shows:
  - original preview
  - extracted text
  - stable page navigation
- DB stores extraction output:
  - page index
  - text
  - method (pdfâ€‘text / docx / visionâ€‘later)
  - confidence + flags

---

## ğŸ”œ M4 â€” Student detail cockpit
**Outcome**
- `/students/[id]` becomes the operational cockpit
- Shows student identity + submission history + latest outcomes

**Acceptance**
- Student page shows:
  - profile basics
  - table of submissions (most recent first)
  - filters (assignment / status / date)
  - click-through to `/submissions/[submissionId]`

---

## ğŸŸ¨ M5 â€” Grading engine v1 (Explainable JSON)
**Outcome**
- Strict perâ€‘criterion decisions with evidence pointers
- Overall word grade calculated from criteria
- Feedback derived from structured decisions (not freestyle)

**Acceptance**
- Given a submission + bound criteria:
  - produce structured JSON
  - store model + prompt version
  - store evidence mapping to pages/snippets

**Current state (2026-02-18)**
- Structured grading JSON is live in submission assessments.
- Evidence-linked criterion decisions are rendered in `Audit & outputs`.
- Grade vocabulary is constrained to Pearson HN path.

---

## ğŸŸ¨ M6 â€” Marked PDF generator
**Outcome**
- Annotated PDF linked to criteria decisions
- Original layout preserved

**Acceptance**
- Downloadable marked PDF attached to submission record
- Annotation log stored for audit

**Current state (2026-02-18)**
- Marked PDF generation is live in grading/rebuild flows.
- Overall summary is placed on final page.
- Constructive page notes are mapped from criterion evidence pages.
- Note payload/config are stored in assessment JSON for audit replay.

---

## ğŸ”œ M7 â€” Export packs
**Outcome**
- Oneâ€‘click export pack per submission:
  - authoritative JSON
  - marked PDF
  - optional CSV summary
  - optional ZIP

**Acceptance**
- Export is deterministic and repeatable
- Past exports can be regenerated identically (versions logged)

---

## Maintenance rule
Update milestone status only when you can point to:
- the UI path that proves it
- the DB tables/fields that store it
- the audit event or log that would defend it
